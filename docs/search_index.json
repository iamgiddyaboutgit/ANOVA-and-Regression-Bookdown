[["anova-fun.html", "Chapter 6 ANOVA Fundamentals 6.1 Law of Total Variance 6.2 Partitioning the SS 6.3 Step 1: Make up Data 6.4 Checking the Assumptions 6.5 Regression and Categorical Variables", " Chapter 6 ANOVA Fundamentals Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures used to compare means. It may be difficult for the neophyte to understand, but ANOVA compares means by analyzing variances. ANOVA is based on the Law of Total Variance (), where the observed variance in a particular variable is partitioned into components attributable to different sources of variation.3 6.1 Law of Total Variance The law of total variance, also known as EVE’s law45, is very important for understanding how ANOVA works. The law says that for random variables \\(X\\) and \\(Y\\) on the same probability space, we have \\[\\begin{equation} \\label{eqn:EVE} \\mathrm{Var}(Y) = \\mathrm{E}[\\mathrm{Var}(Y|X)] + \\mathrm{Var}[\\mathrm{E}(Y|X)]. \\end{equation}\\] Let us consider the case of One-way ANOVA using Fixed Effects. We observe the random variable \\(Y_{ij}\\), the \\(j^{th}\\) response for the \\(i^{th}\\) level of the single factor. Let \\(A_i\\) be the event where the \\(i^{th}\\) level of this factor is observed. Then can be interpreted as \\[\\begin{equation} \\label{eqn:EVE2} \\mathrm{Var}(Y_{ij}) = \\underbrace{\\mathrm{E}[\\mathrm{Var}(Y_{ij}|A_i)]}_{\\substack{\\text{variance of } Y \\\\ \\text{within groups}}} + \\underbrace{\\mathrm{Var}[\\mathrm{E}(Y_{ij}|A_i)]}_{\\substack{\\text{variance of } Y \\\\ \\text{between groups}}} \\end{equation}\\] Can be simplified into something computationally useful for our ANOVA? \\[\\begin{equation} \\label{eqn:EVE2.1} \\mathrm{E}(Y_{ij}|A_i) = \\sum_{j = 1}^{n_i} y_{ij} \\frac{1}{n_i} = \\bar{y}_{i \\cdot} \\end{equation}\\] \\[\\begin{equation} \\label{eqn:EVE2.2} \\mathrm{Var}(Y_{ij}|A_i) = \\sum_{j = 1}^{n_i} \\left(y_{ij} - \\bar{y}_{i \\cdot} \\right)^2 \\frac{1}{n_i} \\end{equation}\\] \\[\\begin{equation} \\label{eqn:EVE2.3} \\sum_{i=1}^{k} \\sum_{j = 1}^{n_i} \\left( y_{ij} - \\bar{\\bar{y}} \\right)^2 \\frac{1}{N} = \\sum_{i=1}^{k} \\left( \\sum_{j = 1}^{n_i} \\left(y_{ij} - \\bar{y}_{i \\cdot} \\right)^2 \\frac{1}{n_i} \\right) \\frac{n_i}{N} + \\sum_{i=1}^{k} \\left( \\bar{y}_{i \\cdot} - \\bar{\\bar{y}} \\right)^2 \\frac{n_i}{N} \\end{equation}\\] After multiplying both sides by \\(N\\), we have this partition of the \\(\\text{SS}\\) \\[\\begin{equation} \\label{eqn:EVE2.4} \\sum_{i=1}^{k} \\sum_{j = 1}^{n_i} \\left( y_{ij} - \\bar{\\bar{y}} \\right)^2 = \\sum_{i=1}^{k} \\sum_{j = 1}^{n_i} \\left(y_{ij} - \\bar{y}_{i \\cdot} \\right)^2 + \\sum_{i=1}^{k} n_i\\left( \\bar{y}_{i \\cdot} - \\bar{\\bar{y}} \\right)^2 \\end{equation}\\] Recall6 the definition of the mean of a population. Let \\(X\\) be a random variable with a finite number of finite outcomes Recall78 the definition of the expected value of a continuous random variable conditioned on an event. Recall9 that the population variance of a finite population of size \\(N\\) with values \\(y_i\\) and mean \\(\\mu\\) is \\[\\begin{equation} \\label{eqn:pop_var} \\sigma^2 = \\frac{1}{N} \\underbrace{\\sum_{i = 1}^{N} \\left(y_i-\\mu\\right)^2}_{\\text{SS}} \\end{equation}\\] where the sum is known as a sum of squares (SS). 6.2 Partitioning the SS source(file.path(&quot;src&quot;, &quot;get-SS.R&quot;)) Let’s look at this simulated experiment to see the SS Decomposition in action. How can we explain the variation that we see in the response? source(file.path(&quot;src&quot;, &quot;partitioning-SS.R&quot;)) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Figure 6.1: Responses were simulated for five different levels of a single factor. The respective sample sizes for each treatment group were 10, 14, 13, 9, and 8 . \\[\\text{SS}_{\\text{total}} = \\sum_{i=1}^{k} \\sum_{j = 1}^{n_i} \\left( y_{ij} - \\bar{\\bar{y}} \\right)^2 = 1501.907\\] \\[\\text{SS}_{\\text{within}} = \\sum_{i=1}^{k} \\sum_{j = 1}^{n_i} \\left(y_{ij} - \\bar{y}_{i \\cdot} \\right)^2 = 399.904\\] \\[\\text{SS}_{\\text{between}} = \\sum_{i=1}^{k} n_i\\left( \\bar{y}_{i \\cdot} - \\bar{\\bar{y}} \\right)^2 = 1102.004\\] Notice SS that \\(\\text{SS}_{\\text{total}} = \\text{SS}_{\\text{within}} + \\text{SS}_{\\text{between}}\\) The total sum of squares divided by the total degrees of freedom is the total variance in the response variable. library(cellWise) library(knitr) opts_chunk$set(tidy.opts=list(width.cutoff=50),tidy=TRUE) 6.3 Step 1: Make up Data # dataset1 6.4 Checking the Assumptions After running your ANOVA, check that the assumptions about the errors are met so that you can do statistical inference. Those assumptions are: \\(\\text{E}(\\epsilon_{ij})=0,\\ \\text{Var}(\\epsilon_{ij})=\\sigma_{i}^2 &lt; \\infty,\\ \\text{for all }i, j.\\) The \\(\\epsilon_{ij}\\) are mutually independent and normally distributed. \\(\\sigma_{i}^2=\\sigma^2\\ \\text{for all } i.\\) 6.4.1 Checking Assumption 1 6.4.2 Assumption 1 was violated. 6.4.3 Checking Assumption 2 6.4.4 Assumption 2 was violated. 6.4.5 Checking Assumption 3 6.4.6 Assumption 3 was violated. A variance-stabilizing transformation of the response variable may help. data(&quot;data_mortality&quot;) transformed_response = transfo(data_mortality, prestandardize = FALSE) ## ## The input data has 198 rows and 91 columns. hist(data_mortality[, 1]) hist(transformed_response$Xt[, 1]) shapiro.test(data_mortality[, 1]) ## ## Shapiro-Wilk normality test ## ## data: data_mortality[, 1] ## W = 0.86877, p-value = 4.552e-12 shapiro.test(transformed_response$Xt[, 1]) ## ## Shapiro-Wilk normality test ## ## data: transformed_response$Xt[, 1] ## W = 0.88041, p-value = 1.968e-11 6.5 Regression and Categorical Variables library(tidymodels) ## Registered S3 method overwritten by &#39;tune&#39;: ## method from ## required_pkgs.model_spec parsnip ## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ── ## ✓ broom 0.7.6 ✓ rsample 0.1.0 ## ✓ dials 0.0.9 ✓ tibble 3.1.2 ## ✓ ggplot2 3.3.3 ✓ tidyr 1.1.3 ## ✓ infer 0.5.4 ✓ tune 0.1.5 ## ✓ modeldata 0.1.0 ✓ workflows 0.2.2 ## ✓ parsnip 0.1.6 ✓ workflowsets 0.0.2 ## ✓ purrr 0.3.4 ✓ yardstick 0.0.8 ## ✓ recipes 0.1.16 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## x purrr::discard() masks scales::discard() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x recipes::step() masks stats::step() ## • Use tidymodels_prefer() to resolve common conflicts. library(ggplot2) There is a profound connection between linear regression and ANOVA. In order to see this, you have to understand that the categorical variables of an ANOVA can be coded with numbers, which allows them to be used in a linear regression model. Let us recall10 the multiple linear regression model. Given a random sample of \\(n\\) observations \\((Y_{i}, X_{i1}, . . ., X_{ip}),\\ i=1,...,n\\), the basic multiple linear regression model is \\[ Y_{i}=\\beta_0+\\beta_1X_{i1}+...+\\beta_pX_{ip}+\\epsilon_i,\\quad i=1,...,n \\] where each \\(\\epsilon_i\\) is a random variable with a mean of \\(0\\). In matrix form, this can be written as \\[ \\begin{bmatrix} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_n \\end{bmatrix} = \\begin{bmatrix} 1 &amp; X_{1,1} &amp; X_{1,2} &amp; \\dots &amp; X_{1, p}\\\\ 1 &amp; X_{2,1} &amp; X_{2,2} &amp; \\dots &amp; X_{2, p}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; X_{n,1} &amp; X_{n,2} &amp; \\dots &amp; X_{n, p}\\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_0\\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_p\\\\ \\end{bmatrix} + \\begin{bmatrix} \\epsilon_0\\\\ \\epsilon_1\\\\ \\vdots\\\\ \\epsilon_n\\\\ \\end{bmatrix} \\] Here, the \\(X_{i,j}\\) represent our coded categorical variables. These categorical variables are coded according to the hypotheses of interest. In many cases, the coding is done so that the newly coded variables are contrasts of the old categorical variables. A contrast is a linear combination of variables such that the coefficients sum to 0. \\[\\sum_i{a_i\\theta_i}\\quad\\text{such that}\\quad\\sum_i{a_i}=0\\] Unlike in ANOVA, in regression, it is best to use coding schemes based on orthogonal and fractional contrasts. Orthogonal contrasts are a set of contrasts in which, for any distinct pair, the sum of the cross-products of the coefficients is 0. \\[ \\sum_i{a_ib_i}=0 \\] I believe that a fractional contrast is such that \\[ \\sum_i{|a_i|}=2 \\] Categorical variable coding schemes can be easily expressed in a matrix format. The convention is to have the old categorical variables as the row headers and the newly coded variables as the column headers. In such a matrix, the \\([c_{ij}]\\) entry indicates the value of the \\(j^{th}\\) level of the new variable for the \\(i^{th}\\) level of the old variable. Here is an example of such a matrix constructed using orthogonal and fractional contrasts. (contr_mat = matrix(data = c(1, 0, -1, 0.5, -1, 0.5), nrow = 3, ncol = 2)) ## [,1] [,2] ## [1,] 1 0.5 ## [2,] 0 -1.0 ## [3,] -1 0.5 Interpreting this coding scheme in the context of our linear model, we see that \\[ \\begin{aligned} E(Y_i|X_{i1}=1,X_{i2}=\\tfrac{1}{2}) &amp;= \\beta_0+\\beta_1+\\tfrac{1}{2}\\beta_2 &amp;= \\mu_1 \\\\ E(Y_i|X_{i1}=0,X_{i2}=-1) &amp;= \\beta_0-\\beta_2 &amp;= \\mu_2\\\\ E(Y_i|X_{i1}=-1,X_{i2}=\\tfrac{1}{2}) &amp;= \\beta_0-\\beta_1+\\tfrac{1}{2}\\beta_2 &amp;= \\mu_3 \\end{aligned} \\] or, in matrix format, \\[ \\begin{bmatrix} 1 &amp; 1 &amp; \\tfrac{1}{2} \\\\ 1 &amp; 0 &amp; -1 \\\\ 1 &amp; -1 &amp; \\tfrac{1}{2} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix} = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix} \\] We can solve this for \\(\\boldsymbol{\\beta}\\) for interpretation’s sake. solve(cbind(rep(1, nrow(contr_mat)), contr_mat)) ## [,1] [,2] [,3] ## [1,] 0.3333333 0.3333333 0.3333333 ## [2,] 0.5000000 0.0000000 -0.5000000 ## [3,] 0.3333333 -0.6666667 0.3333333 Let’s look at another contrast matrix and see if we can interpret it. contr.helmert(n = 3) ## [,1] [,2] ## 1 -1 -1 ## 2 1 -1 ## 3 0 2 solve(cbind(rep(1, 3), contr.helmert(n = 3))) ## 1 2 3 ## [1,] 0.3333333 0.3333333 0.3333333 ## [2,] -0.5000000 0.5000000 0.0000000 ## [3,] -0.1666667 -0.1666667 0.3333333 Perhaps you have heard of polynomial regression? Polynomial regression is just a special case of linear regression in a different basis. In polynomial regression, (just like multiple linear regression) if you use all of your explanatory variables, then you will likely get multi-collinearity problems. contr.poly(n = 3) ## .L .Q ## [1,] -7.071068e-01 0.4082483 ## [2,] -7.850462e-17 -0.8164966 ## [3,] 7.071068e-01 0.4082483 (A = solve(cbind(rep(1, 3), contr.poly(n = 3)))) ## [,1] [,2] [,3] ## 0.3333333 0.3333333 0.3333333 ## .L -0.7071068 0.0000000 0.7071068 ## .Q 0.4082483 -0.8164966 0.4082483 The first matrix shows how to code the levels of your categorical variable and the second matrix is used for interpretation. For example, we can test whether the difference between the means from two populations are equal by doing a linear regression or an ANOVA. Let’s make up some data and try it! source(file.path(&quot;src&quot;, &quot;fabricate.R&quot;)) design = data.frame(group = c(0, 1), n = c(10, 10)) data1 = fabricate(flr = design) Let’s check out our data. # Make a linear model data1_lm_independent_samples = lm(response ~ group, data = data1) # plot ggplot(data = data1, aes(x = group, y = response, color = factor(group))) + geom_boxplot() + geom_jitter(height = 0, width = 0.1) + geom_abline(intercept = data1_lm_independent_samples$coefficients[1], slope = data1_lm_independent_samples$coefficients[2]) + labs(title = &quot;Group Comparison from a Regression Standpoint&quot;, color = &quot;Group&quot;, x = &quot;Group&quot;, y = &quot;Response&quot;) + scale_x_discrete(limits = c(0, 1)) ## Warning: Continuous limits supplied to discrete scale. ## Did you mean `limits = factor(...)` or `scale_*_continuous()`? The way you code your categorical variables in a linear model is extremely important. Different codings lead to different interpretations of the parameters (betas) in your model. For us, our model is \\[ Y_i = \\beta_0+\\beta_{i1}X_{i1}+\\epsilon_i \\] From this, we have \\[ \\begin{aligned} E(Y_i|X_{i1}=0) &amp;=\\beta_0 \\\\ E(Y_i|X_{i1}=1) &amp;=\\beta_0 + \\beta_1 \\end{aligned} \\] From which we can derive, \\[ \\beta_1 = E(Y_i|X_{i1}=1) - E(Y_i|X_{i1}=0) \\] So, our slope estimate is the estimated amount by which the mean of group1 is above that of the mean of group0. Run linear regression summary(data1_lm_independent_samples) ## ## Call: ## lm(formula = response ~ group, data = data1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.191 -1.845 -0.534 1.432 5.193 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.1370 0.7942 40.466 &lt;2e-16 *** ## group -2.9160 1.1231 -2.596 0.0182 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.511 on 18 degrees of freedom ## Multiple R-squared: 0.2725, Adjusted R-squared: 0.232 ## F-statistic: 6.741 on 1 and 18 DF, p-value: 0.01824 Run ANOVA data1$group = as.factor(data1$group) data1_ANOVA_independent_samples = aov(response ~ group, data = data1) summary(data1_ANOVA_independent_samples) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 1 42.52 42.52 6.741 0.0182 * ## Residuals 18 113.53 6.31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Run t-Test (data1_t_test_independent_samples = t.test(x = data1[data1$group == 1, &quot;response&quot;], y = data1[data1$group == 0, &quot;response&quot;], paired = FALSE, var.equal = TRUE)) ## ## Two Sample t-test ## ## data: data1[data1$group == 1, &quot;response&quot;] and data1[data1$group == 0, &quot;response&quot;] ## t = -2.5963, df = 18, p-value = 0.01824 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -5.2755962 -0.5564038 ## sample estimates: ## mean of x mean of y ## 29.221 32.137 Notice the similarities. # Confidence interval for the difference in the # means confint(data1_lm_independent_samples, &quot;group&quot;, level = 0.95) ## 2.5 % 97.5 % ## group -5.275596 -0.5564038 data1_t_test_independent_samples$conf.int ## [1] -5.2755962 -0.5564038 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 # p-values with(summary(data1_lm_independent_samples), unname(pf(fstatistic[1], fstatistic[2], fstatistic[3], lower.tail = F))) ## [1] 0.01823956 summary(data1_ANOVA_independent_samples)[[1]][[1, 5]] ## [1] 0.01823956 data1_t_test_independent_samples$p.value ## [1] 0.01823956 Now, let’s look at something else. The CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. data(&quot;CO2&quot;) CO2[sample(nrow(CO2), size = 5), ] ## Plant Type Treatment conc uptake ## 77 Mc2 Mississippi chilled 1000 14.4 ## 50 Mn2 Mississippi nonchilled 95 12.0 ## 30 Qc2 Quebec chilled 175 27.3 ## 46 Mn1 Mississippi nonchilled 350 30.0 ## 78 Mc3 Mississippi chilled 95 10.6 What is a linear model? In the context of linear regression, a linear model is a relationship between the responses and the explanatory variables that is linear in the parameters. CO2_recipe = recipe(uptake ~ ., data = CO2) %&gt;% step_dummy(c(&quot;Type&quot;, &quot;Treatment&quot;)) # see contrasts() function CO2_linear_model = linear_reg() %&gt;% set_engine(&quot;lm&quot;, contrasts = list(Plant = &quot;contr.poly&quot;)) CO2_workflow = workflow() %&gt;% add_model(CO2_linear_model) %&gt;% add_recipe(CO2_recipe) CO2_fit = CO2_workflow %&gt;% fit(data = CO2) CO2_fit %&gt;% pull_workflow_fit() %&gt;% tidy() ## # A tibble: 15 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 19.5 1.17 16.7 2.96e-26 ## 2 Plant.L -22.9 2.27 -10.1 2.17e-15 ## 3 Plant.Q -4.62 2.27 -2.03 4.57e- 2 ## 4 Plant.C 4.67 2.27 2.06 4.34e- 2 ## 5 Plant^4 2.34 2.27 1.03 3.06e- 1 ## 6 Plant^5 4.31 2.27 1.90 6.13e- 2 ## 7 Plant^6 -0.0390 2.27 -0.0172 9.86e- 1 ## 8 Plant^7 -2.04 2.27 -0.897 3.73e- 1 ## 9 Plant^8 -3.28 2.27 -1.44 1.53e- 1 ## 10 Plant^9 -9.07 2.27 -4.00 1.56e- 4 ## 11 Plant^10 0.546 2.27 0.241 8.10e- 1 ## 12 Plant^11 1.91 2.27 0.843 4.02e- 1 ## 13 conc 0.0177 0.00223 7.96 1.97e-11 ## 14 Type_Mississippi NA NA NA NA ## 15 Treatment_chilled NA NA NA NA 1. Xie, Y. Bookdown: Authoring books and technical documents with r markdown. (2021). 2. Xie, Y. Dynamic documents with R and knitr. (Chapman; Hall/CRC, 2015). 3. Analysis of variance. https://en.wikipedia.org/wiki/Analysis_of_variance. 4. Law of total variance. https://en.wikipedia.org/wiki/Law_of_total_variance. 5. Law of total variance intuition. https://math.stackexchange.com/a/3377007. 6. Expected value. https://en.wikipedia.org/wiki/Expected_value#Finite_case. 7. L09.2 conditioning a continuous random variable on an event. https://www.youtube.com/watch?v=mHj4A1gh_ws. 8. Casella, G. &amp; Berger, R. L. Statistical inference. 150 (Brooks/Cole Cengage Learning). 9. Variance. https://en.wikipedia.org/wiki/Variance#Population_variance. 10. Linear model. https://en.wikipedia.org/wiki/Linear_model. References 3. Analysis of variance. https://en.wikipedia.org/wiki/Analysis_of_variance. 4. Law of total variance. https://en.wikipedia.org/wiki/Law_of_total_variance. 5. Law of total variance intuition. https://math.stackexchange.com/a/3377007. 6. Expected value. https://en.wikipedia.org/wiki/Expected_value#Finite_case. 7. L09.2 conditioning a continuous random variable on an event. https://www.youtube.com/watch?v=mHj4A1gh_ws. 8. Casella, G. &amp; Berger, R. L. Statistical inference. 150 (Brooks/Cole Cengage Learning). 9. Variance. https://en.wikipedia.org/wiki/Variance#Population_variance. 10. Linear model. https://en.wikipedia.org/wiki/Linear_model. "]]
